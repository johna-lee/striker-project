import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import os
import re
from urllib.parse import urlparse

def create_folder_from_url(url):
    # Parse the URL
    parsed_url = urlparse(url)
    # Get the path part
    path = parsed_url.path
    
    # Extract match identifier from the URL
    match_id = re.search(r'/matches/([^/]+)/', path)
    
    if match_id:
        # Use match ID as part of folder name
        folder_name = f"fbref_match_{match_id.group(1)}"
    else:
        # Fallback: Use the last part of the path
        path_parts = path.strip('/').split('/')
        folder_name = f"fbref_{'_'.join(path_parts[-2:])}"
    
    # Replace any invalid characters for folder names
    folder_name = re.sub(r'[\\/*?:"<>|]', "_", folder_name)
    
    # Create the folder if it doesn't exist
    if not os.path.exists(folder_name):
        os.makedirs(folder_name)
        print(f"Created folder: {folder_name}")
    else:
        print(f"Folder already exists: {folder_name}")
    
    return folder_name

def extract_match_id(url):
    """
    Extract the match ID from the URL
    """
    match = re.search(r'/matches/([^/]+)/', url)
    if match:
        return match.group(1)
    return None

def scrape_fbref_tables(url):
    # Add headers to avoid being blocked
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }
    
    # Make the request
    response = requests.get(url, headers=headers)
    
    # Check if request was successful
    if response.status_code != 200:
        print(f"Failed to retrieve page: Status code {response.status_code}")
        return None
    
    # Parse HTML content
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Find all tables
    tables = soup.find_all('table')
    
    # Dictionary to store all table data
    all_tables = {}
    
    # Process each table
    for i, table in enumerate(tables):
        # Get table header
        thead = table.find('thead')
        tbody = table.find('tbody')
        
        # Skip if table doesn't have both thead and tbody
        if not thead or not tbody:
            continue
            
        # Get table ID or create a generic name
        table_id = table.get('id', f'table_{i+1}')
        
        # Extract header rows
        header_rows = []
        for tr in thead.find_all('tr'):
            header_row = []
            for th in tr.find_all(['th', 'td']):
                # Get colspan value (default to 1 if not specified)
                colspan = int(th.get('colspan', 1))
                # Add cell content to row (repeated if colspan > 1)
                header_row.extend([th.text.strip()] * colspan)
            header_rows.append(header_row)
            
        # Extract body rows
        body_rows = []
        for tr in tbody.find_all('tr'):
            body_row = []
            for td in tr.find_all(['td', 'th']):
                # Handle colspan similar to headers
                colspan = int(td.get('colspan', 1))
                body_row.extend([td.text.strip()] * colspan)
            body_rows.append(body_row)
            
        # Store the data
        all_tables[table_id] = {
            'header_rows': header_rows,
            'body_rows': body_rows
        }
    
    return all_tables

def save_tables_to_csv(all_tables, folder_name, match_id):
    saved_files = []
    for table_id, table_data in all_tables.items():
        # If there are multiple header rows, use the last one as column names
        header = table_data['header_rows'][-1] if table_data['header_rows'] else []
        
        # Create DataFrame
        df = pd.DataFrame(table_data['body_rows'], columns=header if header else None)
        
        # Add match_id column to each row
        df.insert(0, 'match_id', match_id)
        print(f"Added match_id column '{match_id}' to table {table_id}")
        
        # Process the Nation column if it exists
        if 'Nation' in df.columns:
            df['Nation'] = df['Nation'].apply(lambda x: x.split(' ', 1)[1] if ' ' in x else x)
            print(f"Processed 'Nation' column in table {table_id} - extracted text after space")
        
        # Save to CSV in the specified folder
        filename = os.path.join(folder_name, f"{table_id}.csv")
        df.to_csv(filename, index=False)
        print(f"Saved table {table_id} to {filename}")
        saved_files.append(filename)
    
    return saved_files

def delete_non_summary_files(folder_name):
    """
    Delete all files in the given folder that don't contain 'summary' in their filename.
    
    Args:
        folder_name (str): Path to the folder containing files to check
    """
    deleted_count = 0
    kept_count = 0
    
    # List all files in the folder
    for filename in os.listdir(folder_name):
        file_path = os.path.join(folder_name, filename)
        
        # Skip directories
        if os.path.isdir(file_path):
            continue
        
        # Check if file contains 'summary' in its name
        if 'summary' not in filename.lower():
            try:
                os.remove(file_path)
                print(f"Deleted: {file_path}")
                deleted_count += 1
            except Exception as e:
                print(f"Error deleting {file_path}: {e}")
        else:
            print(f"Kept: {file_path}")
            kept_count += 1
    
    print(f"\nCleanup summary: Deleted {deleted_count} files, kept {kept_count} summary files.")

# URL of the match
url = "https://fbref.com/en/matches/07f058d4/Dinamo-Zagreb-Chelsea-September-6-2022-Champions-League"

# Extract match ID from URL
match_id = extract_match_id(url)
if match_id:
    print(f"Extracted match ID: {match_id}")
else:
    print("Could not extract match ID from URL, using 'unknown' instead")
    match_id = "unknown"

# Create folder based on URL
folder_name = create_folder_from_url(url)

# Scrape tables
print("Scraping tables from FBref...")
all_tables = scrape_fbref_tables(url)

if all_tables:
    print(f"Found {len(all_tables)} tables with thead and tbody elements.")
    
    # Print the table IDs
    print("Table IDs:")
    for table_id in all_tables.keys():
        print(f" - {table_id}")
    
    # Save tables to CSV in the created folder with match ID column
    saved_files = save_tables_to_csv(all_tables, folder_name, match_id)
    
    # Delete files that don't contain "summary" in their names
    print("\nCleaning up files - keeping only 'summary' files...")
    delete_non_summary_files(folder_name)
else:
    print("No tables found or scraping failed.")